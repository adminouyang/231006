import os
import re
import requests
import subprocess
from urllib.parse import urlparse, urlunparse
from ipaddress import ip_address, IPv4Address, IPv6Address
import concurrent.futures
import time
import threading
from collections import OrderedDict
import ssl
import socket
import hashlib

# é…ç½®å‚æ•°
CONFIG_DIR = 'py/ä¼˜è´¨æº/config'
SUBSCRIBE_FILE = os.path.join(CONFIG_DIR, 'subscribe.txt')
DEMO_FILE = os.path.join(CONFIG_DIR, 'demo.txt')
LOCAL_FILE = os.path.join(CONFIG_DIR, 'local.txt')
BLACKLIST_FILE = os.path.join(CONFIG_DIR, 'blacklist.txt')
RUN_COUNT_FILE = os.path.join(CONFIG_DIR, 'run_count.txt')  # æ–°å¢è¿è¡Œæ¬¡æ•°è®°å½•æ–‡ä»¶

OUTPUT_DIR = 'py/ä¼˜è´¨æº/output'
IPV4_DIR = os.path.join(OUTPUT_DIR, 'ipv4')
IPV6_DIR = os.path.join(OUTPUT_DIR, 'ipv6')
SPEED_LOG = os.path.join(OUTPUT_DIR, 'sort.log')

SPEED_TEST_DURATION = 5
MAX_WORKERS = 10
HTTPS_VERIFY = False
SPEED_THRESHOLD = 120  # æ–°å¢é€Ÿåº¦é˜ˆå€¼120KB/s[6](@ref)
RESET_COUNT = 12       # æ–°å¢è¿è¡Œ12æ¬¡åé‡ç½®é»‘åå•[7](@ref)

# å…¨å±€å˜é‡
failed_domains = set()
log_lock = threading.Lock()
domain_lock = threading.Lock()
counter_lock = threading.Lock()
url_cache_lock = threading.Lock()
url_cache = set()  # å…¨å±€URLç¼“å­˜ç”¨äºå»é‡

os.makedirs(IPV4_DIR, exist_ok=True)
os.makedirs(IPV6_DIR, exist_ok=True)


# --------------------------
# æ–°å¢è¿è¡Œæ¬¡æ•°ç®¡ç†å‡½æ•°
# --------------------------
def manage_run_count():
    """ç®¡ç†è¿è¡Œæ¬¡æ•°å¹¶åœ¨ç¬¬12æ¬¡æ—¶æ¸…ç©ºé»‘åå•[7](@ref)"""
    try:
        # è¯»å–å½“å‰è¿è¡Œæ¬¡æ•°
        if os.path.exists(RUN_COUNT_FILE):
            with open(RUN_COUNT_FILE, 'r') as f:
                current_count = int(f.read().strip())
        else:
            current_count = 0
        
        # å¢åŠ è¿è¡Œæ¬¡æ•°
        current_count += 1
        print(f"ğŸ”¢ å½“å‰æ˜¯ç¬¬ {current_count} æ¬¡è¿è¡Œ")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç©ºé»‘åå•[8](@ref)
        if current_count >= RESET_COUNT:
            print("ğŸ”„ è¾¾åˆ°12æ¬¡è¿è¡Œï¼Œæ¸…ç©ºé»‘åå•æ–‡ä»¶")
            if os.path.exists(BLACKLIST_FILE):
                with open(BLACKLIST_FILE, 'w') as f:
                    f.write('')  # æ¸…ç©ºæ–‡ä»¶å†…å®¹
                print("âœ… é»‘åå•æ–‡ä»¶å·²æ¸…ç©º")
            current_count = 0  # é‡ç½®è®¡æ•°å™¨
        
        # ä¿å­˜æ›´æ–°åçš„è¿è¡Œæ¬¡æ•°
        with open(RUN_COUNT_FILE, 'w') as f:
            f.write(str(current_count))
        
        return current_count
        
    except Exception as e:
        print(f"âŒ è¿è¡Œæ¬¡æ•°ç®¡ç†é”™è¯¯: {str(e)}")
        return 1


# --------------------------
# å·¥å…·å‡½æ•°
# --------------------------
def write_log(message):
    """çº¿ç¨‹å®‰å…¨çš„æ—¥å¿—å†™å…¥"""
    with log_lock:
        with open(SPEED_LOG, 'a', encoding='utf-8') as f:
            f.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}\n")


def get_domain(url):
    """æå–åŸŸå"""
    try:
        netloc = urlparse(url).netloc
        return netloc.split(':')[0] if ':' in netloc else netloc
    except:
        return None


def normalize_url(url):
    """æ ‡å‡†åŒ–URLï¼Œå»é™¤å¤šä½™å‚æ•°ï¼Œç”¨äºå»é‡æ¯”è¾ƒ"""
    try:
        parsed = urlparse(url)
        # ä¿ç•™åŸºæœ¬éƒ¨åˆ†ï¼šåè®®ã€åŸŸåã€ç«¯å£ã€è·¯å¾„
        # å»æ‰æŸ¥è¯¢å‚æ•°å’Œç‰‡æ®µ
        normalized = urlunparse((
            parsed.scheme,
            parsed.netloc,
            parsed.path.rstrip('/') if parsed.path else '/',
            '',  # params
            '',  # query
            ''   # fragment
        ))
        return normalized
    except:
        return url


def get_url_hash(url):
    """è·å–URLçš„å“ˆå¸Œå€¼ï¼Œç”¨äºå¿«é€Ÿæ¯”è¾ƒ"""
    normalized = normalize_url(url)
    return hashlib.md5(normalized.encode('utf-8')).hexdigest()


def is_duplicate_url(url):
    """æ£€æŸ¥URLæ˜¯å¦é‡å¤"""
    url_hash = get_url_hash(url)
    with url_cache_lock:
        if url_hash in url_cache:
            return True
        url_cache.add(url_hash)
        return False


def update_blacklist(domain):
    """æ›´æ–°é»‘åå•"""
    if domain:
        with domain_lock:
            failed_domains.add(domain)


def get_ip_type(url):
    """å®‰å…¨è·å–IPç±»å‹"""
    try:
        host = urlparse(url).hostname
        if not host:
            return 'ipv4'

        # å°è¯•è§£æIPåœ°å€ç±»å‹
        ip = ip_address(host)
        return 'ipv6' if isinstance(ip, IPv6Address) else 'ipv4'
    except ValueError:
        return 'ipv4'
    except Exception as e:
        print(f"âš ï¸ IPç±»å‹æ£€æµ‹å¼‚å¸¸: {str(e)} â† {url}")
        return 'ipv4'


def get_protocol(url):
    """è·å–URLçš„åè®®ç±»å‹"""
    try:
        return urlparse(url).scheme.lower()
    except:
        return 'unknown'


def test_https_certificate(domain, port=443):
    """æµ‹è¯•HTTPSè¯ä¹¦æœ‰æ•ˆæ€§"""
    try:
        context = ssl.create_default_context()
        with socket.create_connection((domain, port), timeout=5) as sock:
            with context.wrap_socket(sock, server_hostname=domain) as ssock:
                cert = ssock.getpeercert()
                # æ£€æŸ¥è¯ä¹¦æœ‰æ•ˆæœŸ
                not_after = cert.get('notAfter', '')
                if not_after:
                    # ç®€å•éªŒè¯è¯ä¹¦æ˜¯å¦åœ¨æœ‰æ•ˆæœŸå†…
                    return True, "è¯ä¹¦æœ‰æ•ˆ"
                return True, "è¯ä¹¦ä¿¡æ¯è·å–æˆåŠŸ"
    except ssl.SSLError as e:
        return False, f"SSLé”™è¯¯: {str(e)}"
    except Exception as e:
        return False, f"è¯ä¹¦æ£€æŸ¥å¤±è´¥: {str(e)}"
    return False, "æœªçŸ¥é”™è¯¯"


# --------------------------
# æ ¸å¿ƒé€»è¾‘
# --------------------------
def parse_demo_file():
    """è§£æé¢‘é“æ¨¡æ¿æ–‡ä»¶"""
    print("\nğŸ” è§£æé¢‘é“æ¨¡æ¿æ–‡ä»¶...")
    alias_map = {}
    group_map = {}
    group_order = []
    channel_order = OrderedDict()
    current_group = None

    try:
        with open(DEMO_FILE, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue

                if line.endswith(',#genre#'):
                    current_group = line.split(',', 1)[0]
                    group_order.append(current_group)
                    channel_order[current_group] = []
                    print(f"  å‘ç°åˆ†ç»„ [{current_group}]")
                elif current_group and line:
                    parts = [p.strip() for p in line.split('|')]
                    standard_name = parts[0]
                    channel_order[current_group].append(standard_name)

                    for alias in parts:
                        alias_map[alias] = standard_name
                    group_map[standard_name] = current_group

        print(f"âœ… å‘ç° {len(group_order)} ä¸ªåˆ†ç»„ï¼Œ{len(alias_map)} ä¸ªåˆ«å")
        return alias_map, group_map, group_order, channel_order

    except Exception as e:
        print(f"âŒ æ¨¡æ¿è§£æå¤±è´¥: {str(e)}")
        return {}, {}, [], OrderedDict()


def fetch_sources():
    """è·å–è®¢é˜…æºæ•°æ®"""
    print("\nğŸ” è·å–è®¢é˜…æº...")
    sources = []

    try:
        with open(SUBSCRIBE_FILE, 'r') as f:
            urls = [line.strip() for line in f if line.strip()]

        print(f"  å‘ç° {len(urls)} ä¸ªè®¢é˜…åœ°å€")
        for idx, url in enumerate(urls, 1):
            try:
                print(f"\nğŸŒ æ­£åœ¨è·å–æº ({idx}/{len(urls)})ï¼š{url}")
                response = requests.get(url, timeout=15, verify=HTTPS_VERIFY)
                content = response.text

                if '#EXTM3U' in content or url.endswith('.m3u'):
                    parsed = parse_m3u(content)
                    print(f"  è§£æåˆ° {len(parsed)} ä¸ªM3Uæº")
                    sources.extend(parsed)
                else:
                    parsed = parse_txt(content)
                    print(f"  è§£æåˆ° {len(parsed)} ä¸ªTXTæº")
                    sources.extend(parsed)

            except Exception as e:
                print(f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}")

    except FileNotFoundError:
        print("âš ï¸ è®¢é˜…æ–‡ä»¶ä¸å­˜åœ¨")

    return sources


def parse_m3u(content):
    """è§£æM3Uæ ¼å¼å†…å®¹"""
    channels = []
    current = None
    
    for line in content.splitlines():
        line = line.strip()
        
        if not line:  # è·³è¿‡ç©ºè¡Œ
            continue
            
        if line.startswith('#EXTINF:'):
            # è§£æEXTINFè¡Œ
            name_match = re.search(r'tvg-name="([^"]*)"', line)
            name = name_match.group(1) if name_match else 'æœªçŸ¥é¢‘é“'
            
            # å¯ä»¥æå–æ›´å¤šå±æ€§
            logo_match = re.search(r'tvg-logo="([^"]*)"', line)
            group_match = re.search(r'group-title="([^"]*)"', line)
            
            # ä»é€—å·åæå–æ˜¾ç¤ºåç§°ï¼ˆå¦‚æœæœ‰ï¼‰
            display_name = line.split(',')[-1] if ',' in line else name
            
            current = {
                'name': name,
                'display_name': display_name,
                'logo': logo_match.group(1) if logo_match else '',
                'group': group_match.group(1) if group_match else '',
                'urls': []
            }
            
        elif line and not line.startswith('#'):
            # URLè¡Œ
            if current is not None:
                current['urls'].append(line)
                # å¦‚æœæœ‰å¤šä¸ªURLï¼Œä¸ç«‹å³æ·»åŠ ï¼Œç­‰ä¸‹ä¸€ä¸ªEXTINFè¡Œæˆ–æ–‡ä»¶ç»“æŸ
            else:
                # æ²¡æœ‰EXTINFä¿¡æ¯çš„URLï¼Œè·³è¿‡æˆ–å¤„ç†ä¸ºåŒ¿åé¢‘é“
                pass
                
        elif line.startswith('#EXTM3U'):
            # æ–‡ä»¶å¤´ï¼Œå¯ä»¥å¤„ç†ç‰ˆæœ¬ä¿¡æ¯ç­‰
            continue
            
        elif line.startswith('#EXTGRP:'):
            # å¤„ç†åˆ†ç»„ä¿¡æ¯
            if current is not None:
                current['group'] = line.split(':', 1)[1]
                
    # å¤„ç†æœ€åä¸€ä¸ªé¢‘é“
    if current is not None and current['urls']:
        channels.append(current)
        
    # å±•å¼€å¤šä¸ªURL
    result = []
    for channel in channels:
        for url in channel['urls']:
            result.append({
                'name': channel['name'],
                'display_name': channel.get('display_name', channel['name']),
                'logo': channel.get('logo', ''),
                'group': channel.get('group', ''),
                'url': url
            })
    
    return result


def parse_txt(content):
    """è§£æTXTæ ¼å¼å†…å®¹"""
    channels = []
    for line in content.split('\n'):
        line = line.strip()
        if ',' in line:
            try:
                name, urls = line.split(',', 1)
                for url in urls.split('#'):
                    clean_url = url.split('$')[0].strip()
                    if clean_url:
                        channels.append({'name': name.strip(), 'url': clean_url})
            except Exception as e:
                print(f"âŒ è§£æå¤±è´¥: {str(e)} â† {line}")
    return channels


def parse_local():
    """è§£ææœ¬åœ°æºæ–‡ä»¶"""
    print("\nğŸ” è§£ææœ¬åœ°æº...")
    sources = []
    try:
        with open(LOCAL_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if ',' in line:
                    try:
                        name, urls = line.split(',', 1)
                        for url in urls.split('#'):
                            parts = url.split('$', 1)
                            source = {
                                'name': name.strip(),
                                'url': parts[0].strip(),
                                'whitelist': len(parts) > 1
                            }
                            sources.append(source)
                    except Exception as e:
                        print(f"âŒ è§£æå¤±è´¥: {str(e)} â† {line}")
        print(f"âœ… æ‰¾åˆ° {len(sources)} ä¸ªæœ¬åœ°æº")
    except FileNotFoundError:
        print("âš ï¸ æœ¬åœ°æºæ–‡ä»¶ä¸å­˜åœ¨")
    return sources


def read_blacklist():
    """è¯»å–é»‘åå•åˆ—è¡¨"""
    try:
        with open(BLACKLIST_FILE, 'r') as f:
            return [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        return []


def filter_sources(sources, blacklist):
    """è¿‡æ»¤é»‘åå•æº"""
    print("\nğŸ” è¿‡æ»¤é»‘åå•...")
    filtered = []
    blacklist_lower = [kw.lower() for kw in blacklist]

    for s in sources:
        # URLæ ¼å¼æ ¡éªŒ
        if not urlparse(s['url']).scheme:
            print(f"ğŸš« æ— æ•ˆURLæ ¼å¼: {s['url']}")
            continue

        if s.get('whitelist', False):
            filtered.append(s)
            continue

        if any(kw in s['url'].lower() for kw in blacklist_lower):
            print(f"ğŸš« æ‹¦æˆªé»‘åå•: {s['url']}")
            continue

        filtered.append(s)

    print(f"âœ… ä¿ç•™ {len(filtered)}/{len(sources)} ä¸ªæº")
    return filtered


def test_rtmp(url):
    """RTMPæ¨æµæ£€æµ‹"""
    try:
        result = subprocess.run(
            ['ffmpeg', '-i', url, '-t', '1', '-v', 'error', '-f', 'null', '-'],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            timeout=10
        )
        if result.returncode == 0:
            write_log(f"RTMPæ£€æµ‹æˆåŠŸ: {url}")
            return 100
        write_log(f"RTMPæ£€æµ‹å¤±è´¥: {url} | {result.stderr.decode()[:100]}")
        return 0
    except Exception as e:
        write_log(f"RTMPæ£€æµ‹å¼‚å¸¸: {url} | {str(e)}")
        return 0


def test_https_specific(url, domain):
    """HTTPSåè®®ç‰¹æ®Šæ£€æµ‹"""
    try:
        # æµ‹è¯•è¯ä¹¦æœ‰æ•ˆæ€§
        cert_valid, cert_msg = test_https_certificate(domain)
        
        # è¿›è¡Œå¸¸è§„é€Ÿåº¦æµ‹è¯•
        start_time = time.time()
        with requests.Session() as session:
            response = session.get(url,
                                   stream=True,
                                   timeout=(3.05, 5),
                                   allow_redirects=True,
                                   verify=HTTPS_VERIFY,
                                   headers={'User-Agent': 'Mozilla/5.0'})

            total_bytes = 0
            data_start = time.time()
            for chunk in response.iter_content(chunk_size=1024 * 1024):
                if chunk:
                    total_bytes += len(chunk)
                if (time.time() - data_start) >= SPEED_TEST_DURATION:
                    break

            duration = max(time.time() - data_start, 0.001)
            speed = (total_bytes / 1024) / duration
            
            # è®°å½•HTTPSç‰¹å®šä¿¡æ¯
            https_info = f" | è¯ä¹¦: {'æœ‰æ•ˆ' if cert_valid else 'æ— æ•ˆ'}"
            log_msg = (f"âœ… HTTPSæµ‹é€ŸæˆåŠŸ: {url}\n"
                       f"   é€Ÿåº¦: {speed:.2f}KB/s | æ•°æ®é‡: {total_bytes / 1024:.1f}KB | "
                       f"æ€»è€—æ—¶: {time.time() - start_time:.2f}s{https_info}")
            write_log(log_msg)
            return speed
            
    except requests.exceptions.SSLError as e:
        log_msg = f"âŒ HTTPS SSLé”™è¯¯: {url} | é”™è¯¯: {str(e)}"
        write_log(log_msg)
        return 0
    except Exception as e:
        domain = get_domain(url)
        update_blacklist(domain)
        log_msg = f"âŒ HTTPSæµ‹é€Ÿå¤±è´¥: {url} | é”™è¯¯: {str(e)}"
        write_log(log_msg)
        return 0


def test_speed(url):
    """å¢å¼ºç‰ˆæµ‹é€Ÿå‡½æ•°ï¼Œæ”¯æŒHTTPSæ£€æµ‹"""
    try:
        protocol = get_protocol(url)
        
        # RTMPåè®®å¤„ç†
        if protocol in ['rtmp', 'rtmps']:
            return test_rtmp(url)

        # HTTPSåè®®ç‰¹æ®Šå¤„ç†
        if protocol == 'https':
            domain = get_domain(url)
            if domain:
                return test_https_specific(url, domain)
            else:
                write_log(f"âš ï¸ æ— æ³•æå–HTTPSåŸŸå: {url}")
                return 0

        # HTTPåè®®å¤„ç†
        if protocol not in ['http', 'https']:
            write_log(f"âš ï¸ è·³è¿‡éå¸¸è§„åè®®: {url}")
            return 0

        # æ™®é€šHTTPè¯·æ±‚
        start_time = time.time()
        with requests.Session() as session:
            response = session.get(url,
                                   stream=True,
                                   timeout=(3.05, 5),
                                   allow_redirects=True,
                                   verify=False,
                                   headers={'User-Agent': 'Mozilla/5.0'})

            total_bytes = 0
            data_start = time.time()
            for chunk in response.iter_content(chunk_size=1024 * 1024):
                if chunk:
                    total_bytes += len(chunk)
                if (time.time() - data_start) >= SPEED_TEST_DURATION:
                    break

            duration = max(time.time() - data_start, 0.001)
            speed = (total_bytes / 1024) / duration
            
            # æ–°å¢é€Ÿåº¦é˜ˆå€¼æ£€æŸ¥[6](@ref)
            if speed > SPEED_THRESHOLD:
                status = "âœ… é€šè¿‡é˜ˆå€¼"
            else:
                status = "ğŸš« æœªè¾¾é˜ˆå€¼"
                
            log_msg = (f"{status} {protocol.upper()}æµ‹é€Ÿ: {url}\n"
                       f"   é€Ÿåº¦: {speed:.2f}KB/s | æ•°æ®é‡: {total_bytes / 1024:.1f}KB | "
                       f"æ€»è€—æ—¶: {time.time() - start_time:.2f}s | é˜ˆå€¼: {SPEED_THRESHOLD}KB/s")
            write_log(log_msg)
            return speed

    except Exception as e:
        domain = get_domain(url)
        update_blacklist(domain)
        protocol = get_protocol(url)
        log_msg = (f"âŒ {protocol.upper()}æµ‹é€Ÿå¤±è´¥: {url}\n"
                   f"   é”™è¯¯: {str(e)} | åŸŸå: {domain}")
        write_log(log_msg)
        return 0


def process_sources(sources):
    """å¤„ç†æ‰€æœ‰æºå¹¶è¿›è¡Œæµ‹é€Ÿï¼Œåº”ç”¨é€Ÿåº¦é˜ˆå€¼è¿‡æ»¤[6](@ref)"""
    total = len(sources)
    print(f"\nğŸ” å¼€å§‹æ£€æµ‹ {total} ä¸ªæº")
    print(f"ğŸ“Š é€Ÿåº¦é˜ˆå€¼: {SPEED_THRESHOLD}KB/s")
    
    processed = []
    processed_count = 0
    passed_count = 0  # é€šè¿‡é˜ˆå€¼è®¡æ•°
    duplicate_count = 0  # é‡å¤URLè®¡æ•°

    # ç»Ÿè®¡åè®®ç±»å‹
    protocol_stats = {}
    seen_urls = set()  # ç”¨äºå»é‡çš„URLé›†åˆ

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {}
        for s in sources:
            # åœ¨æäº¤ä»»åŠ¡å‰æ£€æŸ¥URLæ˜¯å¦é‡å¤
            url_hash = get_url_hash(s['url'])
            if url_hash in seen_urls:
                duplicate_count += 1
                print(f"â­ï¸  è·³è¿‡é‡å¤URL: {s['name']} | {s['url'][:50]}...")
                continue
            seen_urls.add(url_hash)
            
            future = executor.submit(
                lambda s: (s['name'], s['url'], test_speed(s['url']), 
                          get_ip_type(s['url']), get_protocol(s['url'])), s)
            futures[future] = s

        for future in concurrent.futures.as_completed(futures):
            try:
                name, url, speed, ip_type, protocol = future.result()
                with counter_lock:
                    processed_count += 1
                    progress = f"[{processed_count}/{total}]"

                    # æ›´æ–°åè®®ç»Ÿè®¡
                    if protocol not in protocol_stats:
                        protocol_stats[protocol] = {'total': 0, 'passed': 0}
                    protocol_stats[protocol]['total'] += 1

                speed_str = f"{speed:>7.2f}KB/s".rjust(12)
                protocol_icon = "ğŸ”’" if protocol == "https" else "ğŸŒ"
                if protocol in ['rtmp', 'rtmps']:
                    protocol_icon = "ğŸ“¹"
                
                # åº”ç”¨é€Ÿåº¦é˜ˆå€¼è¿‡æ»¤[6](@ref)
                if speed > SPEED_THRESHOLD:
                    status = "âœ…"
                    passed_count += 1
                    protocol_stats[protocol]['passed'] += 1
                    
                    # å†æ¬¡æ£€æŸ¥URLé‡å¤ï¼ˆå¤šçº¿ç¨‹ç¯å¢ƒä¸‹ï¼‰
                    if not is_duplicate_url(url):
                        processed.append((name, url, speed, ip_type, protocol))
                    else:
                        print(f"ğŸ” æ£€æµ‹åˆ°é‡å¤URLï¼ˆå·²è·³è¿‡ï¼‰: {name} | {url[:50]}...")
                        passed_count -= 1
                else:
                    status = "âŒ"
                
                print(f"{progress} {status} é¢‘é“: {name[:15]:<15} | é€Ÿåº¦:{speed_str} | {protocol_icon}{protocol.upper()} | {url}")
                
            except Exception as e:
                print(f"âš ï¸ å¤„ç†å¼‚å¸¸: {str(e)}")

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š é€Ÿåº¦é˜ˆå€¼è¿‡æ»¤ç»“æœ:")
    print(f"   ğŸ“¡ æ€»æ£€æµ‹æ•°: {processed_count}")
    print(f"   ğŸ” è·³è¿‡é‡å¤: {duplicate_count}")
    print(f"   âœ… é€šè¿‡æ•°: {passed_count} (é€Ÿåº¦ > {SPEED_THRESHOLD}KB/s)")
    print(f"   âŒ æ·˜æ±°æ•°: {processed_count - passed_count} (é€Ÿåº¦ â‰¤ {SPEED_THRESHOLD}KB/s)")
    print(f"   ğŸ“ˆ é€šè¿‡ç‡: {passed_count/max(processed_count,1)*100:.1f}%")
    
    print(f"ğŸ“Š åè®®åˆ†å¸ƒ:")
    for protocol, data in protocol_stats.items():
        icon = "ğŸ”’" if protocol == "https" else ("ğŸ“¹" if protocol in ['rtmp', 'rtmps'] else "ğŸŒ")
        passed = data['passed']
        total = data['total']
        pass_rate = passed/max(total,1)*100
        print(f"   {icon} {protocol.upper():<6}: {passed}/{total} é€šè¿‡ ({pass_rate:.1f}%)")

    # ä¿å­˜é»‘åå•æ›´æ–°
    if failed_domains:
        existing = set()
        if os.path.exists(BLACKLIST_FILE):
            with open(BLACKLIST_FILE, 'r') as f:
                existing = set(line.strip() for line in f)

        new_domains = failed_domains - existing
        if new_domains:
            with open(BLACKLIST_FILE, 'a') as f:
                for domain in new_domains:
                    f.write(f"{domain}\n")
            print(f"ğŸ†• æ–°å¢ {len(new_domains)} ä¸ªåŸŸååˆ°é»‘åå•")

    print(f"\nâœ… å…¨éƒ¨æºæ£€æµ‹å®Œæˆï¼Œæœ€ç»ˆä¿ç•™ {len(processed)} ä¸ªä¸é‡å¤çš„æœ‰æ•ˆæº")
    return processed


def organize_channels(processed, alias_map, group_map):
    """æ•´ç†é¢‘é“æ•°æ®ï¼Œå¹¶è¿›è¡ŒURLå»é‡"""
    print("\nğŸ“š æ•´ç†é¢‘é“æ•°æ®...")
    organized = {'ipv4': OrderedDict(), 'ipv6': OrderedDict()}
    duplicate_stats = {'total': 0, 'channel': {}}

    for name, url, speed, ip_type, protocol in processed:
        if ip_type not in ('ipv4', 'ipv6'):
            print(f"âš ï¸ å¼‚å¸¸IPç±»å‹: {ip_type}ï¼Œä½¿ç”¨ipv4ä»£æ›¿ â† {url}")
            ip_type = 'ipv4'

        std_name = alias_map.get(name, name)
        group = group_map.get(std_name, 'å…¶ä»–')

        if group not in organized[ip_type]:
            organized[ip_type][group] = OrderedDict()
        if std_name not in organized[ip_type][group]:
            organized[ip_type][group][std_name] = []

        # æ£€æŸ¥åŒä¸€é¢‘é“ä¸‹æ˜¯å¦æœ‰é‡å¤URL
        existing_urls = {normalize_url(u[0]) for u in organized[ip_type][group][std_name]}
        normalized_url = normalize_url(url)
        
        if normalized_url in existing_urls:
            # æ‰¾åˆ°é‡å¤çš„æºï¼Œä¿ç•™é€Ÿåº¦æ›´å¿«çš„
            for i, (existing_url, existing_speed, existing_protocol) in enumerate(organized[ip_type][group][std_name]):
                if normalize_url(existing_url) == normalized_url:
                    if speed > existing_speed:
                        # ç”¨æ›´å¿«çš„æ›¿æ¢
                        organized[ip_type][group][std_name][i] = (url, speed, protocol)
                        duplicate_stats['total'] += 1
                        duplicate_stats['channel'][std_name] = duplicate_stats['channel'].get(std_name, 0) + 1
                        print(f"ğŸ”„ é¢‘é“ '{std_name}' æ›¿æ¢é‡å¤URL: æ–°é€Ÿåº¦ {speed:.1f}KB/s > æ—§é€Ÿåº¦ {existing_speed:.1f}KB/s")
                    break
        else:
            # æ²¡æœ‰é‡å¤ï¼Œæ·»åŠ æ–°URL
            organized[ip_type][group][std_name].append((url, speed, protocol))

    # æ‰“å°å»é‡ç»Ÿè®¡
    if duplicate_stats['total'] > 0:
        print(f"ğŸ” é¢‘é“å†…å»é‡: å…±æ¸…ç† {duplicate_stats['total']} ä¸ªé‡å¤æº")
        if len(duplicate_stats['channel']) <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ªé¢‘é“
            for channel, count in duplicate_stats['channel'].items():
                print(f"   ğŸ“º {channel}: {count} ä¸ªé‡å¤æº")
        else:
            print(f"   ğŸ“º æ¶‰åŠ {len(duplicate_stats['channel'])} ä¸ªé¢‘é“")
    
    # å¯¹æ¯ä¸ªé¢‘é“çš„æºæŒ‰é€Ÿåº¦æ’åº
    for ip_type in ['ipv4', 'ipv6']:
        for group in organized[ip_type]:
            for channel in organized[ip_type][group]:
                organized[ip_type][group][channel].sort(key=lambda x: x[1], reverse=True)
    
    return organized


def deduplicate_final_output(txt_lines, m3u_lines):
    """å¯¹æœ€ç»ˆè¾“å‡ºè¿›è¡Œå»é‡"""
    print("\nğŸ” å¯¹æœ€ç»ˆè¾“å‡ºè¿›è¡Œå»é‡...")
    
    # å¯¹TXTæ ¼å¼å»é‡
    txt_dict = {}
    txt_duplicates = 0
    for line in txt_lines:
        if line.endswith(',#genre#'):
            txt_dict[line] = line
        elif ',' in line:
            channel, url = line.split(',', 1)
            key = f"{channel},{normalize_url(url)}"
            if key not in txt_dict:
                txt_dict[key] = line
            else:
                txt_duplicates += 1
    
    deduped_txt = list(txt_dict.values())
    
    # å¯¹M3Uæ ¼å¼å»é‡
    m3u_dict = {}
    m3u_duplicates = 0
    i = 0
    while i < len(m3u_lines):
        if m3u_lines[i].startswith('#EXTINF:'):
            if i + 1 < len(m3u_lines) and not m3u_lines[i + 1].startswith('#'):
                extinf_line = m3u_lines[i]
                url_line = m3u_lines[i + 1]
                key = normalize_url(url_line)
                if key not in m3u_dict:
                    m3u_dict[key] = (extinf_line, url_line)
                else:
                    m3u_duplicates += 1
                i += 2
            else:
                i += 1
        else:
            i += 1
    
    # é‡æ–°æ„å»ºM3Uæ–‡ä»¶
    deduped_m3u = ['#EXTM3U x-tvg-url="https://gh.catmak.name/https://raw.githubusercontent.com/Guovin/iptv-api/refs/heads/master/output/epg/epg.gz"']
    for extinf, url in m3u_dict.values():
        deduped_m3u.append(extinf)
        deduped_m3u.append(url)
    
    if txt_duplicates > 0 or m3u_duplicates > 0:
        print(f"âœ… å»é‡å®Œæˆ: ç§»é™¤ {txt_duplicates} ä¸ªé‡å¤TXTè¡Œï¼Œ{m3u_duplicates} ä¸ªé‡å¤M3Uæº")
    
    return deduped_txt, deduped_m3u


def finalize_output(organized, group_order, channel_order):
    """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶ - åˆå¹¶æ‰€æœ‰åè®®åˆ°å•ä¸€æ–‡ä»¶ï¼Œå¹¶è¿›è¡Œå»é‡"""
    print("\nğŸ“‚ ç”Ÿæˆç»“æœæ–‡ä»¶...")
    
    for ip_type in ['ipv4', 'ipv6']:
        txt_lines = []
        m3u_lines = [
            '#EXTM3U x-tvg-url="https://gh.catmak.name/https://raw.githubusercontent.com/Guovin/iptv-api/refs/heads/master/output/epg/epg.gz"',
        ]

        # ç»Ÿè®¡ä¿¡æ¯
        total_sources = 0
        speed_stats = []
        seen_channels = set()

        # æŒ‰æ¨¡æ¿é¡ºåºå¤„ç†åˆ†ç»„
        for group in group_order:
            if group not in organized[ip_type]:
                continue

            txt_lines.append(f"{group},#genre#")

            # å¤„ç†æ¨¡æ¿é¢‘é“
            for channel in channel_order[group]:
                if channel not in organized[ip_type][group]:
                    continue

                # åˆå¹¶æ‰€æœ‰åè®®çš„æºï¼ŒæŒ‰é€Ÿåº¦æ’åº
                all_urls = organized[ip_type][group][channel]
                urls = sorted(all_urls, key=lambda x: x[1], reverse=True)
                
                # å¯¹æ¯ä¸ªURLï¼Œåªä¿ç•™æœ€ä½³ï¼ˆå·²æ’åºï¼Œç¬¬ä¸€ä¸ªæœ€å¿«ï¼‰
                seen_in_channel = set()
                unique_urls = []
                for url, speed, protocol in urls:
                    normalized_url = normalize_url(url)
                    if normalized_url not in seen_in_channel:
                        seen_in_channel.add(normalized_url)
                        unique_urls.append((url, speed, protocol))
                
                # ç”ŸæˆTXTæ ¼å¼ï¼šæ¯ä¸ªURLå•ç‹¬ä¸€è¡Œ
                for url, speed, protocol in unique_urls:
                    txt_lines.append(f"{channel},{url}")
                    total_sources += 1
                    speed_stats.append(speed)
                    
                    # ç”ŸæˆM3Uæ ¼å¼
                    protocol_icon = "ğŸ”’" if protocol == "https" else "ğŸ“¹" if protocol in ['rtmp', 'rtmps'] else "ğŸŒ"
                    m3u_lines.append(f'#EXTINF:-1 tvg-name="{channel}" tvg-logo="https://gh.catmak.name/https://raw.githubusercontent.com/fanmingming/live/main/tv/{channel}.png" group-title="{group}",{protocol_icon} {channel} | {speed:.1f}KB/s')
                    m3u_lines.append(url)
                
                seen_channels.add(channel)

            # å¤„ç†é¢å¤–é¢‘é“
            extra = sorted(
                [c for c in organized[ip_type][group] if c not in channel_order[group]],
                key=lambda x: x.lower()
            )
            for channel in extra:
                all_urls = organized[ip_type][group][channel]
                urls = sorted(all_urls, key=lambda x: x[1], reverse=True)
                
                # å»é‡
                seen_in_channel = set()
                unique_urls = []
                for url, speed, protocol in urls:
                    normalized_url = normalize_url(url)
                    if normalized_url not in seen_in_channel:
                        seen_in_channel.add(normalized_url)
                        unique_urls.append((url, speed, protocol))
                
                if unique_urls:
                    for url, speed, protocol in unique_urls:
                        txt_lines.append(f"{channel},{url}")
                        total_sources += 1
                        speed_stats.append(speed)
                        
                        protocol_icon = "ğŸ”’" if protocol == "https" else "ğŸ“¹" if protocol in ['rtmp', 'rtmps'] else "ğŸŒ"
                        m3u_lines.append(f'#EXTINF:-1 tvg-name="{channel}" tvg-logo="https://gh.catmak.name/https://raw.githubusercontent.com/fanmingming/live/main/tv/{channel}.png" group-title="{group}",{protocol_icon} {channel} | {speed:.1f}KB/s')
                        m3u_lines.append(url)
                    
                    seen_channels.add(channel)

        # å¤„ç†å…¶ä»–åˆ†ç»„
        if 'å…¶ä»–' in organized[ip_type]:
            txt_lines.append("å…¶ä»–,#genre#")
            for channel in sorted(organized[ip_type]['å…¶ä»–'].keys(), key=lambda x: x.lower()):
                if channel in seen_channels:
                    continue
                    
                all_urls = organized[ip_type]['å…¶ä»–'][channel]
                urls = sorted(all_urls, key=lambda x: x[1], reverse=True)
                
                # å»é‡
                seen_in_channel = set()
                unique_urls = []
                for url, speed, protocol in urls:
                    normalized_url = normalize_url(url)
                    if normalized_url not in seen_in_channel:
                        seen_in_channel.add(normalized_url)
                        unique_urls.append((url, speed, protocol))
                
                if unique_urls:
                    for url, speed, protocol in unique_urls:
                        txt_lines.append(f"{channel},{url}")
                        total_sources += 1
                        speed_stats.append(speed)
                        
                        protocol_icon = "ğŸ”’" if protocol == "https" else "ğŸ“¹" if protocol in ['rtmp', 'rtmps'] else "ğŸŒ"
                        m3u_lines.append(f'#EXTINF:-1 tvg-name="{channel}" tvg-logo="https://gh.catmak.name/https://raw.githubusercontent.com/fanmingming/live/main/tv/{channel}.png" group-title="å…¶ä»–",{protocol_icon} {channel} | {speed:.1f}KB/s')
                        m3u_lines.append(url)
        
        # æœ€ç»ˆå»é‡
        txt_lines, m3u_lines = deduplicate_final_output(txt_lines, m3u_lines)
        
        # å†™å…¥æ–‡ä»¶
        dir_path = IPV4_DIR if ip_type == 'ipv4' else IPV6_DIR
        
        # åªç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶ï¼šresult.txt å’Œ result.m3u
        with open(os.path.join(dir_path, 'result.txt'), 'w', encoding='utf-8') as f:
            f.write('\n'.join(txt_lines))
        with open(os.path.join(dir_path, 'result.m3u'), 'w', encoding='utf-8') as f:
            f.write('\n'.join(m3u_lines))

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if speed_stats:
            avg_speed = sum(speed_stats) / len(speed_stats)
            max_speed = max(speed_stats)
            min_speed = min(speed_stats)
        else:
            avg_speed = max_speed = min_speed = 0

        print(f"âœ… å·²ç”Ÿæˆ {ip_type.upper()} æ–‡ä»¶:")
        print(f"   ğŸ“„ {os.path.join(dir_path, 'result.txt')} - {len(txt_lines)} è¡Œ")
        print(f"   ğŸ“º {os.path.join(dir_path, 'result.m3u')} - {len(m3u_lines)} è¡Œ")
        print(f"   ğŸ“Š ç»Ÿè®¡: {total_sources} ä¸ªæº | å¹³å‡é€Ÿåº¦: {avg_speed:.1f}KB/s")
        print(f"   ğŸ“ˆ é€Ÿåº¦èŒƒå›´: {min_speed:.1f} - {max_speed:.1f}KB/s")
        
        # ç»Ÿè®¡åè®®ä¿¡æ¯
        protocol_count = {}
        for group in organized[ip_type]:
            for channel in organized[ip_type][group]:
                for url, speed, protocol in organized[ip_type][group][channel]:
                    if protocol not in protocol_count:
                        protocol_count[protocol] = 0
                    protocol_count[protocol] += 1
        
        if protocol_count:
            print(f"   ğŸŒ åè®®åˆ†å¸ƒ: {', '.join([f'{p.upper()}:{c}' for p, c in protocol_count.items()])}")


if __name__ == '__main__':
    print("\n" + "=" * 60)
    print("ğŸ¬ IPTVç›´æ’­æºå¤„ç†è„šæœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰")
    print("=" * 60)
    
    # æ–°å¢è¿è¡Œæ¬¡æ•°ç®¡ç†[7](@ref)
    run_count = manage_run_count()
    
    print(f"ğŸ”§ é…ç½®å‚æ•°:")
    print(f"   ğŸ“Š é€Ÿåº¦é˜ˆå€¼: {SPEED_THRESHOLD}KB/s")
    print(f"   ğŸ”¢ è¿è¡Œæ¬¡æ•°: {run_count}/{RESET_COUNT}")
    print(f"   ğŸ” HTTPSè¯ä¹¦éªŒè¯: {'å¼€å¯' if HTTPS_VERIFY else 'å…³é—­'}")
    print(f"   â±ï¸ æµ‹é€Ÿæ—¶é•¿: {SPEED_TEST_DURATION}ç§’")
    print(f"   ğŸ‘¥ æœ€å¤§å¹¶å‘æ•°: {MAX_WORKERS}")
    print(f"   ğŸ“ è¾“å‡ºæ–‡ä»¶: result.txt, result.m3u")
    print(f"   ğŸ” å»é‡åŠŸèƒ½: å·²å¯ç”¨")

    # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
    with open(SPEED_LOG, 'w', encoding='utf-8') as f:
        f.write(f"æµ‹é€Ÿæ—¥å¿— {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"é€Ÿåº¦é˜ˆå€¼: {SPEED_THRESHOLD}KB/s\n")
        f.write(f"è¿è¡Œæ¬¡æ•°: {run_count}\n")
        f.write(f"HTTPSéªŒè¯: {HTTPS_VERIFY}\n\n")

    # æ¸…é™¤URLç¼“å­˜
    url_cache.clear()

    # åˆå§‹åŒ–æ•°æ®
    alias_map, group_map, group_order, channel_order = parse_demo_file()
    sources = fetch_sources() + parse_local()
    blacklist = read_blacklist()

    # å¤„ç†æµç¨‹
    filtered = filter_sources(sources, blacklist)
    processed = process_sources(filtered)
    organized = organize_channels(processed, alias_map, group_map)
    finalize_output(organized, group_order, channel_order)

    print("\n" + "=" * 60)
    print("ğŸ‰ å¤„ç†å®Œæˆï¼")
    print(f"ğŸ”¢ æœ¬æ¬¡è¿è¡Œæ¬¡æ•°: {run_count}")
    if run_count >= RESET_COUNT - 1:
        print(f"âš ï¸ ä¸‹æ¬¡è¿è¡Œå°†æ¸…ç©ºé»‘åå•")
    print("ğŸ“ ç»“æœæ–‡ä»¶:")
    print(f"   IPv4: {IPV4_DIR}/result.txt, result.m3u")
    print(f"   IPv6: {IPV6_DIR}/result.txt, result.m3u")
    print("ğŸ” æ‰€æœ‰åè®®æºå·²åˆå¹¶åˆ°åŒä¸€æ–‡ä»¶ä¸­")
    print("âœ… å»é‡åŠŸèƒ½å·²å¯ç”¨ï¼Œå·²ç§»é™¤é‡å¤çš„URL")
    print("=" * 60)
