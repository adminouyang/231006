import os
import re
import requests
import subprocess
from urllib.parse import urlparse
from ipaddress import ip_address, IPv4Address, IPv6Address
import concurrent.futures
import time
import threading
from collections import OrderedDict

# é…ç½®å‚æ•°
CONFIG_DIR = 'py/ä¼˜è´¨æº/config'
SUBSCRIBE_FILE = os.path.join(CONFIG_DIR, 'subscribe.txt')
DEMO_FILE = os.path.join(CONFIG_DIR, 'demo.txt')
LOCAL_FILE = os.path.join(CONFIG_DIR, 'local.txt')
BLACKLIST_FILE = os.path.join(CONFIG_DIR, 'blacklist.txt')

OUTPUT_DIR = 'py/ä¼˜è´¨æº/output'
IPV4_DIR = os.path.join(OUTPUT_DIR, 'ipv4')
IPV6_DIR = os.path.join(OUTPUT_DIR, 'ipv6')
SPEED_LOG = os.path.join(OUTPUT_DIR, 'sort.log')

SPEED_TEST_DURATION = 5
MAX_WORKERS = 10

# å…¨å±€å˜é‡
failed_domains = set()
log_lock = threading.Lock()
domain_lock = threading.Lock()
counter_lock = threading.Lock()

os.makedirs(IPV4_DIR, exist_ok=True)
os.makedirs(IPV6_DIR, exist_ok=True)


# --------------------------
# å·¥å…·å‡½æ•°
# --------------------------
def write_log(message):
    """çº¿ç¨‹å®‰å…¨çš„æ—¥å¿—å†™å…¥"""
    with log_lock:
        with open(SPEED_LOG, 'a', encoding='utf-8') as f:
            f.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}\n")


def get_domain(url):
    """æå–åŸŸå"""
    try:
        netloc = urlparse(url).netloc
        return netloc.split(':')[0] if ':' in netloc else netloc
    except:
        return None


def update_blacklist(domain):
    """æ›´æ–°é»‘åå•"""
    if domain:
        with domain_lock:
            failed_domains.add(domain)


def get_ip_type(url):
    """å®‰å…¨è·å–IPç±»å‹"""
    try:
        host = urlparse(url).hostname
        if not host:
            return 'ipv4'

        # å°è¯•è§£æIPåœ°å€ç±»å‹
        ip = ip_address(host)
        return 'ipv6' if isinstance(ip, IPv6Address) else 'ipv4'
    except ValueError:
        return 'ipv4'
    except Exception as e:
        print(f"âš ï¸ IPç±»å‹æ£€æµ‹å¼‚å¸¸: {str(e)} â† {url}")
        return 'ipv4'


# --------------------------
# æ ¸å¿ƒé€»è¾‘
# --------------------------
def parse_demo_file():
    """è§£æé¢‘é“æ¨¡æ¿æ–‡ä»¶"""
    print("\nğŸ” è§£æé¢‘é“æ¨¡æ¿æ–‡ä»¶...")
    alias_map = {}
    group_map = {}
    group_order = []
    channel_order = OrderedDict()
    current_group = None

    try:
        with open(DEMO_FILE, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue

                if line.endswith(',#genre#'):
                    current_group = line.split(',', 1)[0]
                    group_order.append(current_group)
                    channel_order[current_group] = []
                    print(f"  å‘ç°åˆ†ç»„ [{current_group}]")
                elif current_group and line:
                    parts = [p.strip() for p in line.split('|')]
                    standard_name = parts[0]
                    channel_order[current_group].append(standard_name)

                    for alias in parts:
                        alias_map[alias] = standard_name
                    group_map[standard_name] = current_group

        print(f"âœ… å‘ç° {len(group_order)} ä¸ªåˆ†ç»„ï¼Œ{len(alias_map)} ä¸ªåˆ«å")
        return alias_map, group_map, group_order, channel_order

    except Exception as e:
        print(f"âŒ æ¨¡æ¿è§£æå¤±è´¥: {str(e)}")
        return {}, {}, [], OrderedDict()


def fetch_sources():
    """è·å–è®¢é˜…æºæ•°æ®"""
    print("\nğŸ” è·å–è®¢é˜…æº...")
    sources = []

    try:
        with open(SUBSCRIBE_FILE, 'r') as f:
            urls = [line.strip() for line in f if line.strip()]

        print(f"  å‘ç° {len(urls)} ä¸ªè®¢é˜…åœ°å€")
        for idx, url in enumerate(urls, 1):
            try:
                print(f"\nğŸŒ æ­£åœ¨è·å–æº ({idx}/{len(urls)})ï¼š{url}")
                response = requests.get(url, timeout=15)
                content = response.text

                if '#EXTM3U' in content or url.endswith('.m3u'):
                    parsed = parse_m3u(content)
                    print(f"  è§£æåˆ° {len(parsed)} ä¸ªM3Uæº")
                    sources.extend(parsed)
                else:
                    parsed = parse_txt(content)
                    print(f"  è§£æåˆ° {len(parsed)} ä¸ªTXTæº")
                    sources.extend(parsed)

            except Exception as e:
                print(f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}")

    except FileNotFoundError:
        print("âš ï¸ è®¢é˜…æ–‡ä»¶ä¸å­˜åœ¨")

    return sources


def parse_m3u(content):
    """è§£æM3Uæ ¼å¼å†…å®¹"""
    channels = []
    current = {}
    for line in content.split('\n'):
        line = line.strip()
        if line.startswith('#EXTINF'):
            match = re.search(r'tvg-name="([^"]*)"', line)
            current = {'name': match.group(1) if match else 'æœªçŸ¥é¢‘é“', 'urls': []}
        elif line and not line.startswith('#'):
            if current:
                current['urls'].append(line)
                channels.append(current)
                current = {}
    return [{'name': c['name'], 'url': u} for c in channels for u in c['urls']]


def parse_txt(content):
    """è§£æTXTæ ¼å¼å†…å®¹"""
    channels = []
    for line in content.split('\n'):
        line = line.strip()
        if ',' in line:
            try:
                name, urls = line.split(',', 1)
                for url in urls.split('#'):
                    clean_url = url.split('$')[0].strip()
                    if clean_url:
                        channels.append({'name': name.strip(), 'url': clean_url})
            except Exception as e:
                print(f"âŒ è§£æå¤±è´¥: {str(e)} â† {line}")
    return channels


def parse_local():
    """è§£ææœ¬åœ°æºæ–‡ä»¶"""
    print("\nğŸ” è§£ææœ¬åœ°æº...")
    sources = []
    try:
        with open(LOCAL_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if ',' in line:
                    try:
                        name, urls = line.split(',', 1)
                        for url in urls.split('#'):
                            parts = url.split('$', 1)
                            source = {
                                'name': name.strip(),
                                'url': parts[0].strip(),
                                'whitelist': len(parts) > 1
                            }
                            sources.append(source)
                    except Exception as e:
                        print(f"âŒ è§£æå¤±è´¥: {str(e)} â† {line}")
        print(f"âœ… æ‰¾åˆ° {len(sources)} ä¸ªæœ¬åœ°æº")
    except FileNotFoundError:
        print("âš ï¸ æœ¬åœ°æºæ–‡ä»¶ä¸å­˜åœ¨")
    return sources


def read_blacklist():
    """è¯»å–é»‘åå•åˆ—è¡¨"""
    try:
        with open(BLACKLIST_FILE, 'r') as f:
            return [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        return []


def filter_sources(sources, blacklist):
    """è¿‡æ»¤é»‘åå•æº"""
    print("\nğŸ” è¿‡æ»¤é»‘åå•...")
    filtered = []
    blacklist_lower = [kw.lower() for kw in blacklist]

    for s in sources:
        # URLæ ¼å¼æ ¡éªŒ
        if not urlparse(s['url']).scheme:
            print(f"ğŸš« æ— æ•ˆURLæ ¼å¼: {s['url']}")
            continue

        if s.get('whitelist', False):
            filtered.append(s)
            continue

        if any(kw in s['url'].lower() for kw in blacklist_lower):
            print(f"ğŸš« æ‹¦æˆªé»‘åå•: {s['url']}")
            continue

        filtered.append(s)

    print(f"âœ… ä¿ç•™ {len(filtered)}/{len(sources)} ä¸ªæº")
    return filtered


def test_rtmp(url):
    """RTMPæ¨æµæ£€æµ‹"""
    try:
        result = subprocess.run(
            ['ffmpeg', '-i', url, '-t', '1', '-v', 'error', '-f', 'null', '-'],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            timeout=10
        )
        if result.returncode == 0:
            write_log(f"RTMPæ£€æµ‹æˆåŠŸ: {url}")
            return 100
        write_log(f"RTMPæ£€æµ‹å¤±è´¥: {url} | {result.stderr.decode()[:100]}")
        return 0
    except Exception as e:
        write_log(f"RTMPæ£€æµ‹å¼‚å¸¸: {url} | {str(e)}")
        return 0


def test_speed(url):
    """å¢å¼ºç‰ˆæµ‹é€Ÿå‡½æ•°"""
    try:
        start_time = time.time()

        # RTMPåè®®å¤„ç†
        if url.startswith(('rtmp://', 'rtmps://')):
            return test_rtmp(url)

        # HTTPåè®®å¤„ç†
        if not url.startswith(('http://', 'https://')):
            write_log(f"âš ï¸ è·³è¿‡éå¸¸è§„åè®®: {url}")
            return 0

        with requests.Session() as session:
            response = session.get(url,
                                   stream=True,
                                   timeout=(3.05, 5),
                                   allow_redirects=True,
                                   verify=False,
                                   headers={'User-Agent': 'Mozilla/5.0'})

            total_bytes = 0
            data_start = time.time()
            for chunk in response.iter_content(chunk_size=1024 * 1024):
                if chunk:
                    total_bytes += len(chunk)
                if (time.time() - data_start) >= SPEED_TEST_DURATION:
                    break

            duration = max(time.time() - data_start, 0.001)
            speed = (total_bytes / 1024) / duration
            log_msg = (f"âœ… æµ‹é€ŸæˆåŠŸ: {url}\n"
                       f"   é€Ÿåº¦: {speed:.2f}KB/s | æ•°æ®é‡: {total_bytes / 1024:.1f}KB | "
                       f"æ€»è€—æ—¶: {time.time() - start_time:.2f}s")
            write_log(log_msg)
            return speed

    except Exception as e:
        domain = get_domain(url)
        update_blacklist(domain)
        log_msg = (f"âŒ æµ‹é€Ÿå¤±è´¥: {url}\n"
                   f"   é”™è¯¯: {str(e)} | åŸŸå: {domain}")
        write_log(log_msg)
        return 0


def process_sources(sources):
    """å¤„ç†æ‰€æœ‰æºå¹¶è¿›è¡Œæµ‹é€Ÿ"""
    total = len(sources)
    print(f"\nğŸ” å¼€å§‹æ£€æµ‹ {total} ä¸ªæº")
    processed = []
    processed_count = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(
            lambda s: (s['name'], s['url'], test_speed(s['url']), get_ip_type(s['url'])), s) for s in sources}

        for future in concurrent.futures.as_completed(futures):
            try:
                name, url, speed, ip_type = future.result()
                with counter_lock:
                    processed_count += 1
                    progress = f"[{processed_count}/{total}]"

                speed_str = f"{speed:>7.2f}KB/s".rjust(12)
                print(f"{progress} ğŸ“Š é¢‘é“: {name[:15]:<5}|é€Ÿåº¦:{speed_str} |{url} ,ç±»å‹: {ip_type.upper()}  ")
                processed.append((name, url, speed, ip_type))
            except Exception as e:
                print(f"âš ï¸ å¤„ç†å¼‚å¸¸: {str(e)}")

    # ä¿å­˜é»‘åå•æ›´æ–°
    if failed_domains:
        existing = set()
        if os.path.exists(BLACKLIST_FILE):
            with open(BLACKLIST_FILE, 'r') as f:
                existing = set(line.strip() for line in f)

        new_domains = failed_domains - existing
        if new_domains:
            with open(BLACKLIST_FILE, 'a') as f:
                for domain in new_domains:
                    f.write(f"{domain}\n")
            print(f"ğŸ†• æ–°å¢ {len(new_domains)} ä¸ªåŸŸååˆ°é»‘åå•")

    print("\nâœ… å…¨éƒ¨æºæ£€æµ‹å®Œæˆ")
    return processed


def organize_channels(processed, alias_map, group_map):
    """æ•´ç†é¢‘é“æ•°æ®"""
    print("\nğŸ“š æ•´ç†é¢‘é“æ•°æ®...")
    organized = {'ipv4': OrderedDict(), 'ipv6': OrderedDict()}

    for name, url, speed, ip_type in processed:
        if ip_type not in ('ipv4', 'ipv6'):
            print(f"âš ï¸ å¼‚å¸¸IPç±»å‹: {ip_type}ï¼Œä½¿ç”¨ipv4ä»£æ›¿ â† {url}")
            ip_type = 'ipv4'

        std_name = alias_map.get(name, name)
        group = group_map.get(std_name, 'å…¶ä»–')

        if group not in organized[ip_type]:
            organized[ip_type][group] = OrderedDict()
        if std_name not in organized[ip_type][group]:
            organized[ip_type][group][std_name] = []

        organized[ip_type][group][std_name].append((url, speed))

    return organized


def finalize_output(organized, group_order, channel_order):
    """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶"""
    print("\nğŸ“‚ ç”Ÿæˆç»“æœæ–‡ä»¶...")
    for ip_type in ['ipv4', 'ipv6']:
        txt_lines = []
        m3u_lines = [
            '#EXTM3U x-tvg-url="https://gh.catmak.name/https://raw.githubusercontent.com/Guovin/iptv-api/refs/heads/master/output/epg/epg.gz"',  # æ·»åŠ EPGåœ°å€
        ]

        # æŒ‰æ¨¡æ¿é¡ºåºå¤„ç†åˆ†ç»„
        for group in group_order:
            if group not in organized[ip_type]:
                continue

            txt_lines.append(f"{group},#genre#")
            #m3u_lines.append(f'#EXTINF:-1 group-title="{group}",{group}\n#genre#')

            # å¤„ç†æ¨¡æ¿é¢‘é“
            for channel in channel_order[group]:
                if channel not in organized[ip_type][group]:
                    continue

                urls = sorted(organized[ip_type][group][channel], key=lambda x: x[1], reverse=True)
                selected = [u[0] for u in urls[:10]]

                # if selected:
                #     txt_lines.append(f"{channel},{'#'.join(selected)}")
                    # ä¿®æ”¹è¿™é‡Œï¼šæ¯ä¸ªURLå•ç‹¬ä¸€è¡Œ
                    for url in selected:
                        txt_lines.append(f"{channel},{url}")
                    for url in selected:
                        m3u_lines.append(f'#EXTINF:-1 tvg-name="{channel}"tvg-logo="https://gh.catmak.name/https://raw.githubusercontent.com/fanmingming/live/main/tv/{channel}.png" group-title="{group}",{channel}\n{url}')

            # å¤„ç†é¢å¤–é¢‘é“
            extra = sorted(
                [c for c in organized[ip_type][group] if c not in channel_order[group]],
                key=lambda x: x.lower()
            )
            for channel in extra:
                urls = sorted(organized[ip_type][group][channel], key=lambda x: x[1], reverse=True)
                selected = [u[0] for u in urls[:10]]
                # if selected:
                #     txt_lines.append(f"{channel},{'#'.join(selected)}")
                # ä¿®æ”¹è¿™é‡Œï¼šæ¯ä¸ªURLå•ç‹¬ä¸€è¡Œ
                    for url in selected:
                        txt_lines.append(f"{channel},{url}")
                    for url in selected:
                        m3u_lines.append(f'#EXTINF:-1 tvg-name="{channel}" group-title="{group}",{channel}\n{url}')

        # å¤„ç†å…¶ä»–åˆ†ç»„
        if 'å…¶ä»–' in organized[ip_type]:
            txt_lines.append("å…¶ä»–,#genre#")
            m3u_lines.append('#EXTINF:-1 group-title="å…¶ä»–",å…¶ä»–\n#genre#')
            for channel in sorted(organized[ip_type]['å…¶ä»–'].keys(), key=lambda x: x.lower()):
                urls = sorted(organized[ip_type]['å…¶ä»–'][channel], key=lambda x: x[1], reverse=True)
                selected = [u[0] for u in urls[:10]]
                if selected:
                    txt_lines.append(f"{channel},{'#'.join(selected)}")
                    for url in selected:
                        m3u_lines.append(f'#EXTINF:-1 tvg-name="{channel}" group-title="å…¶ä»–",{channel}\n{url}')

        # å†™å…¥æ–‡ä»¶
        dir_path = IPV4_DIR if ip_type == 'ipv4' else IPV6_DIR
        with open(os.path.join(dir_path, 'result.txt'), 'w', encoding='utf-8') as f:
            f.write('\n'.join(txt_lines))
        with open(os.path.join(dir_path, 'result.m3u'), 'w', encoding='utf-8') as f:
            f.write('\n'.join(m3u_lines))

        print(f"  å·²ç”Ÿæˆ {ip_type.upper()} æ–‡ä»¶ â†’ {dir_path}")


if __name__ == '__main__':
    print("\n" + "=" * 50)
    print("ğŸ¬ IPTVç›´æ’­æºå¤„ç†è„šæœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰")
    print("=" * 50)

    # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
    with open(SPEED_LOG, 'w') as f:
        f.write(f"æµ‹é€Ÿæ—¥å¿— {time.strftime('%Y-%m-%d %H:%M:%S')}\n")

    # åˆå§‹åŒ–æ•°æ®
    alias_map, group_map, group_order, channel_order = parse_demo_file()
    sources = fetch_sources() + parse_local()
    blacklist = read_blacklist()

    # å¤„ç†æµç¨‹
    filtered = filter_sources(sources, blacklist)
    processed = process_sources(filtered)
    organized = organize_channels(processed, alias_map, group_map)
    finalize_output(organized, group_order, channel_order)

    print("\n" + "=" * 50)
    print("ğŸ‰ å¤„ç†å®Œæˆï¼ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³ output ç›®å½•")
    print("=" * 50)
