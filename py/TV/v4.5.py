import os
import re
import requests
import subprocess
from urllib.parse import urlparse
from ipaddress import ip_address, IPv4Address, IPv6Address
import concurrent.futures
import time
import threading
from collections import OrderedDict
import json

# é…ç½®å‚æ•°
CONFIG_DIR = 'config'
SUBSCRIBE_FILE = os.path.join(CONFIG_DIR, 'subscribe.txt')
DEMO_FILE = os.path.join(CONFIG_DIR, 'demo.txt')
LOCAL_FILE = os.path.join(CONFIG_DIR, 'local.txt')
BLACKLIST_FILE = os.path.join(CONFIG_DIR, 'blacklist.txt')
RUN_COUNTER_FILE = os.path.join(CONFIG_DIR, 'run_counter.txt')

OUTPUT_DIR = 'output'
IPV4_DIR = os.path.join(OUTPUT_DIR, 'ipv4')
IPV6_DIR = os.path.join(OUTPUT_DIR, 'ipv6')
SPEED_LOG = os.path.join(OUTPUT_DIR, 'sort.log')

SPEED_TEST_DURATION = 5
MAX_WORKERS = 10
SPEED_THRESHOLD = 50  # é€Ÿåº¦é˜ˆå€¼ï¼Œå•ä½KB/s

# GitHubä»£ç†åˆ—è¡¨
GITHUB_PROXIES = [


    'https://ghproxy.cc/',

    'https://gh.ddlc.top/',

    'https://gh-proxy.com/'
]

# å…¨å±€å˜é‡
failed_domains = set()
log_lock = threading.Lock()
domain_lock = threading.Lock()
counter_lock = threading.Lock()
domain_cache = {}  # åŸŸåæ£€æµ‹ç¼“å­˜
available_proxy = None  # å¯ç”¨çš„GitHubä»£ç†

os.makedirs(IPV4_DIR, exist_ok=True)
os.makedirs(IPV6_DIR, exist_ok=True)


# --------------------------
# å·¥å…·å‡½æ•°
# --------------------------
def write_log(message):
    """çº¿ç¨‹å®‰å…¨çš„æ—¥å¿—å†™å…¥"""
    with log_lock:
        with open(SPEED_LOG, 'a', encoding='utf-8') as f:
            f.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}\n")


def get_domain(url):
    """æå–åŸŸåå’Œç«¯å£ä½œä¸ºå”¯ä¸€æ ‡è¯†"""
    try:
        parsed = urlparse(url)
        domain = parsed.hostname
        port = parsed.port or (443 if parsed.scheme == 'https' else 80)
        return f"{domain}:{port}"
    except:
        return None


def update_blacklist(domain):
    """æ›´æ–°é»‘åå•"""
    if domain:
        with domain_lock:
            failed_domains.add(domain)


def get_ip_type(url):
    """å®‰å…¨è·å–IPç±»å‹"""
    try:
        host = urlparse(url).hostname
        if not host:
            return 'ipv4'

        # å°è¯•è§£æIPåœ°å€ç±»å‹
        ip = ip_address(host)
        return 'ipv6' if isinstance(ip, IPv6Address) else 'ipv4'
    except ValueError:
        return 'ipv4'
    except Exception as e:
        print(f"âš ï¸ IPç±»å‹æ£€æµ‹å¼‚å¸¸: {str(e)} â† {url}")
        return 'ipv4'


def load_run_counter():
    """åŠ è½½è¿è¡Œè®¡æ•°å™¨"""
    try:
        with open(RUN_COUNTER_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return {"run_count": 0, "last_run": None}


def save_run_counter(counter):
    """ä¿å­˜è¿è¡Œè®¡æ•°å™¨"""
    with open(RUN_COUNTER_FILE, 'w', encoding='utf-8') as f:
        json.dump(counter, f, ensure_ascii=False, indent=2)


def clear_blacklist_if_needed():
    """è¿è¡Œ10æ¬¡åæ¸…ç©ºé»‘åå•"""
    counter = load_run_counter()
    current_time = time.strftime('%Y-%m-%d %H:%M:%S')

    # å¦‚æœæ˜¯åŒä¸€å¤©çš„ä¸åŒè¿è¡Œï¼Œä¸å¢åŠ è®¡æ•°
    last_run_date = counter.get('last_run', '').split(' ')[0] if counter.get('last_run') else ''
    current_date = current_time.split(' ')[0]

    if last_run_date != current_date:
        counter['run_count'] = 0

    counter['run_count'] += 1
    counter['last_run'] = current_time

    if counter['run_count'] >= 10:
        print("ğŸ”„ è¾¾åˆ°10æ¬¡è¿è¡Œï¼Œæ¸…ç©ºé»‘åå•...")
        if os.path.exists(BLACKLIST_FILE):
            os.remove(BLACKLIST_FILE)
        counter['run_count'] = 0

    save_run_counter(counter)
    return counter['run_count']


def test_proxy(proxy_url):
    """æµ‹è¯•GitHubä»£ç†æ˜¯å¦å¯ç”¨"""
    # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„GitHubæ–‡ä»¶è¿›è¡Œæµ‹è¯•
    test_url = "https://raw.githubusercontent.com/octocat/Hello-World/master/README"
    try:
        full_url = proxy_url + test_url
        response = requests.get(full_url, timeout=10, verify=False)
        if response.status_code == 200 and "Hello World" in response.text:
            print(f"âœ… ä»£ç†å¯ç”¨: {proxy_url}")
            return True
    except Exception as e:
        print(f"âŒ ä»£ç†ä¸å¯ç”¨: {proxy_url} - {str(e)}")
    return False


def get_github_proxy():
    """è·å–å¯ç”¨çš„GitHubä»£ç†"""
    global available_proxy

    if available_proxy:
        # æµ‹è¯•å½“å‰ä»£ç†æ˜¯å¦ä»ç„¶å¯ç”¨
        if test_proxy(available_proxy):
            return available_proxy

    print("ğŸ” æ£€æµ‹GitHubä»£ç†...")
    for proxy in GITHUB_PROXIES:
        if test_proxy(proxy):
            available_proxy = proxy
            return proxy

    print("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨ä»£ç†ï¼Œç›´æ¥è®¿é—®GitHub")
    return None


def add_proxy_to_github_url(url):
    """ä¸ºGitHubé“¾æ¥æ·»åŠ ä»£ç†"""
    if 'raw.githubusercontent.com' in url and not url.startswith(tuple(GITHUB_PROXIES)):
        proxy = get_github_proxy()
        if proxy:
            # ç›´æ¥æ‹¼æ¥ä»£ç†å’ŒåŸå§‹URL
            return proxy + url
    return url


# --------------------------
# æ ¸å¿ƒé€»è¾‘
# --------------------------
def parse_demo_file():
    """è§£æé¢‘é“æ¨¡æ¿æ–‡ä»¶"""
    print("\nğŸ” è§£æé¢‘é“æ¨¡æ¿æ–‡ä»¶...")
    alias_map = {}
    group_map = {}
    group_order = []
    channel_order = OrderedDict()
    current_group = None

    try:
        with open(DEMO_FILE, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue

                if line.endswith(',#genre#'):
                    current_group = line.split(',', 1)[0]
                    group_order.append(current_group)
                    channel_order[current_group] = []
                    print(f"  å‘ç°åˆ†ç»„ [{current_group}]")
                elif current_group and line:
                    parts = [p.strip() for p in line.split('|')]
                    standard_name = parts[0]
                    channel_order[current_group].append(standard_name)

                    for alias in parts:
                        alias_map[alias] = standard_name
                    group_map[standard_name] = current_group

        print(f"âœ… å‘ç° {len(group_order)} ä¸ªåˆ†ç»„ï¼Œ{len(alias_map)} ä¸ªåˆ«å")
        return alias_map, group_map, group_order, channel_order

    except Exception as e:
        print(f"âŒ æ¨¡æ¿è§£æå¤±è´¥: {str(e)}")
        return {}, {}, [], OrderedDict()


def fetch_sources():
    """è·å–è®¢é˜…æºæ•°æ®ï¼ˆæ”¯æŒGitHubä»£ç†ï¼‰"""
    print("\nğŸ” è·å–è®¢é˜…æº...")
    sources = []

    try:
        # ä¿®å¤ç¼–ç é—®é¢˜ï¼šå°è¯•å¤šç§ç¼–ç 
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
        urls = []

        for encoding in encodings:
            try:
                with open(SUBSCRIBE_FILE, 'r', encoding=encoding) as f:
                    urls = [line.strip() for line in f if line.strip()]
                print(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè¯»å–è®¢é˜…æ–‡ä»¶")
                break
            except UnicodeDecodeError:
                print(f"âš ï¸ {encoding} ç¼–ç è¯»å–å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ç§ç¼–ç ")
                continue
        else:
            # æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨é”™è¯¯å¿½ç•¥æ¨¡å¼
            with open(SUBSCRIBE_FILE, 'r', encoding='utf-8', errors='ignore') as f:
                urls = [line.strip() for line in f if line.strip()]
            print("âš ï¸ ä½¿ç”¨é”™è¯¯å¿½ç•¥æ¨¡å¼è¯»å–è®¢é˜…æ–‡ä»¶")

        print(f"  å‘ç° {len(urls)} ä¸ªè®¢é˜…åœ°å€")
        for idx, url in enumerate(urls, 1):
            try:
                # å¤„ç†GitHubé“¾æ¥ä»£ç†
                final_url = add_proxy_to_github_url(url)
                print(f"\nğŸŒ æ­£åœ¨è·å–æº ({idx}/{len(urls)})ï¼š{final_url}")

                response = requests.get(final_url, timeout=15)
                content = response.text

                if '#EXTM3U' in content or url.endswith('.m3u'):
                    parsed = parse_m3u(content)
                    print(f"  è§£æåˆ° {len(parsed)} ä¸ªM3Uæº")
                    sources.extend(parsed)
                else:
                    parsed = parse_txt(content)
                    print(f"  è§£æåˆ° {len(parsed)} ä¸ªTXTæº")
                    sources.extend(parsed)

            except Exception as e:
                print(f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}")

    except FileNotFoundError:
        print("âš ï¸ è®¢é˜…æ–‡ä»¶ä¸å­˜åœ¨")

    return sources


def parse_m3u(content):
    """è§£æM3Uæ ¼å¼å†…å®¹"""
    channels = []
    current = {}
    for line in content.split('\n'):
        line = line.strip()
        if line.startswith('#EXTINF'):
            match = re.search(r'tvg-name="([^"]*)"', line)
            current = {'name': match.group(1) if match else 'æœªçŸ¥é¢‘é“', 'urls': []}
        elif line and not line.startswith('#'):
            if current:
                current['urls'].append(line)
                channels.append(current)
                current = {}
    return [{'name': c['name'], 'url': u} for c in channels for u in c['urls']]


def parse_txt(content):
    """è§£æTXTæ ¼å¼å†…å®¹"""
    channels = []
    for line in content.split('\n'):
        line = line.strip()
        if ',' in line:
            try:
                name, urls = line.split(',', 1)
                for url in urls.split('#'):
                    clean_url = url.split('$')[0].strip()
                    if clean_url:
                        channels.append({'name': name.strip(), 'url': clean_url})
            except Exception as e:
                print(f"âŒ è§£æå¤±è´¥: {str(e)} â† {line}")
    return channels


def parse_local():
    """è§£ææœ¬åœ°æºæ–‡ä»¶"""
    print("\nğŸ” è§£ææœ¬åœ°æº...")
    sources = []
    try:
        # ä¿®å¤ç¼–ç é—®é¢˜
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']

        for encoding in encodings:
            try:
                with open(LOCAL_FILE, 'r', encoding=encoding) as f:
                    for line in f:
                        line = line.strip()
                        if ',' in line:
                            try:
                                name, urls = line.split(',', 1)
                                for url in urls.split('#'):
                                    parts = url.split('$', 1)
                                    source = {
                                        'name': name.strip(),
                                        'url': parts[0].strip(),
                                        'whitelist': len(parts) > 1
                                    }
                                    sources.append(source)
                            except Exception as e:
                                print(f"âŒ è§£æå¤±è´¥: {str(e)} â† {line}")
                print(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè¯»å–æœ¬åœ°æº")
                break
            except UnicodeDecodeError:
                continue
        else:
            # æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨é”™è¯¯å¿½ç•¥æ¨¡å¼
            with open(LOCAL_FILE, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    line = line.strip()
                    if ',' in line:
                        try:
                            name, urls = line.split(',', 1)
                            for url in urls.split('#'):
                                parts = url.split('$', 1)
                                source = {
                                    'name': name.strip(),
                                    'url': parts[0].strip(),
                                    'whitelist': len(parts) > 1
                                }
                                sources.append(source)
                        except Exception as e:
                            print(f"âŒ è§£æå¤±è´¥: {str(e)} â† {line}")
            print("âš ï¸ ä½¿ç”¨é”™è¯¯å¿½ç•¥æ¨¡å¼è¯»å–æœ¬åœ°æº")

        print(f"âœ… æ‰¾åˆ° {len(sources)} ä¸ªæœ¬åœ°æº")
    except FileNotFoundError:
        print("âš ï¸ æœ¬åœ°æºæ–‡ä»¶ä¸å­˜åœ¨")
    return sources


def read_blacklist():
    """è¯»å–é»‘åå•åˆ—è¡¨"""
    try:
        # ä¿®å¤ç¼–ç é—®é¢˜
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']

        for encoding in encodings:
            try:
                with open(BLACKLIST_FILE, 'r', encoding=encoding) as f:
                    return [line.strip() for line in f if line.strip()]
            except UnicodeDecodeError:
                continue
        # æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
        return []
    except FileNotFoundError:
        return []


def filter_sources(sources, blacklist):
    """è¿‡æ»¤é»‘åå•æº"""
    print("\nğŸ” è¿‡æ»¤é»‘åå•...")
    filtered = []
    blacklist_lower = [kw.lower() for kw in blacklist]

    for s in sources:
        # URLæ ¼å¼æ ¡éªŒ
        if not urlparse(s['url']).scheme:
            print(f"ğŸš« æ— æ•ˆURLæ ¼å¼: {s['url']}")
            continue

        if s.get('whitelist', False):
            filtered.append(s)
            continue

        if any(kw in s['url'].lower() for kw in blacklist_lower):
            print(f"ğŸš« æ‹¦æˆªé»‘åå•: {s['url']}")
            continue

        filtered.append(s)

    print(f"âœ… ä¿ç•™ {len(filtered)}/{len(sources)} ä¸ªæº")
    return filtered


def group_sources_by_domain(sources):
    """æŒ‰åŸŸåå’Œç«¯å£åˆ†ç»„æº"""
    domain_groups = {}
    for source in sources:
        domain = get_domain(source['url'])
        if not domain:
            continue
        if domain not in domain_groups:
            domain_groups[domain] = []
        domain_groups[domain].append(source)
    return domain_groups


def select_test_channel(sources):
    """é€‰æ‹©æµ‹è¯•é¢‘é“ï¼ˆä¼˜å…ˆé€‰æ‹©åŒ…å«CCTVæˆ–å«è§†çš„é¢‘é“ï¼‰"""
    # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šCCTVé¢‘é“
    for source in sources:
        if 'CCTV' in source['name']:
            return source

    # ç¬¬äºŒä¼˜å…ˆçº§ï¼šå«è§†é¢‘é“
    for source in sources:
        if 'å«è§†' in source['name']:
            return source

    # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šå…¶ä»–é¢‘é“
    return sources[0] if sources else None


def test_rtmp(url):
    """RTMPæ¨æµæ£€æµ‹"""
    try:
        result = subprocess.run(
            ['ffmpeg', '-i', url, '-t', '1', '-v', 'error', '-f', 'null', '-'],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            timeout=10
        )
        if result.returncode == 0:
            write_log(f"RTMPæ£€æµ‹æˆåŠŸ: {url}")
            return 100  # RTMPæˆåŠŸè¿”å›100ï¼Œå¤§äºé˜ˆå€¼
        write_log(f"RTMPæ£€æµ‹å¤±è´¥: {url} | {result.stderr.decode()[:100]}")
        return 0
    except Exception as e:
        write_log(f"RTMPæ£€æµ‹å¼‚å¸¸: {url} | {str(e)}")
        return 0


def test_speed(url):
    """å¢å¼ºç‰ˆæµ‹é€Ÿå‡½æ•° - åŸºäº100KBæ•°æ®æµ‹è¯•"""
    try:
        start_time = time.time()

        # RTMPåè®®å¤„ç†
        if url.startswith(('rtmp://', 'rtmps://')):
            return test_rtmp(url)

        # HTTPåè®®å¤„ç†
        if not url.startswith(('http://', 'https://')):
            write_log(f"âš ï¸ è·³è¿‡éå¸¸è§„åè®®: {url}")
            return 0

        with requests.Session() as session:
            response = session.get(url,
                                   stream=True,
                                   timeout=(3.05, 5),
                                   allow_redirects=True,
                                   verify=False,
                                   headers={'User-Agent': 'Mozilla/5.0'})

            total_bytes = 0
            target_bytes = 100 * 1024  # 100KB
            data_start = time.time()

            for chunk in response.iter_content(chunk_size=8192):  # ä½¿ç”¨8KBå—å¤§å°
                if chunk:
                    total_bytes += len(chunk)
                    # è¾¾åˆ°100KBç«‹å³åœæ­¢
                    if total_bytes >= target_bytes:
                        break

                # æ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…ç½‘ç»œé—®é¢˜å¯¼è‡´é•¿æ—¶é—´é˜»å¡
                if (time.time() - data_start) > 10:  # 10ç§’è¶…æ—¶
                    write_log(f"â° æµ‹é€Ÿè¶…æ—¶: {url}")
                    break

            duration = max(time.time() - data_start, 0.001)

            # å¦‚æœå®é™…è¯»å–ä¸è¶³100KBï¼ŒæŒ‰æ¯”ä¾‹è®¡ç®—é€Ÿåº¦
            if total_bytes < target_bytes:
                write_log(f"âš ï¸ ä»…è¯»å– {total_bytes / 1024:.1f}KB æ•°æ®: {url}")
                # å¯ä»¥è¿”å›0æˆ–æŒ‰å®é™…è¯»å–é‡è®¡ç®—
                # return 0

            speed = (total_bytes / 1024) / duration  # KB/s

            # æ£€æŸ¥é€Ÿåº¦æ˜¯å¦å¤§äºé˜ˆå€¼
            if speed > SPEED_THRESHOLD:
                log_msg = (f"âœ… æµ‹é€ŸæˆåŠŸ: {url}\n"
                           f"   é€Ÿåº¦: {speed:.2f}KB/s > {SPEED_THRESHOLD}KB/s | æ•°æ®é‡: {total_bytes / 1024:.1f}KB | "
                           f"æ€»è€—æ—¶: {time.time() - start_time:.2f}s")
                write_log(log_msg)
                return speed
            else:
                log_msg = (f"âš ï¸ é€Ÿåº¦ä¸è¶³: {url}\n"
                           f"   é€Ÿåº¦: {speed:.2f}KB/s <= {SPEED_THRESHOLD}KB/s | æ•°æ®é‡: {total_bytes / 1024:.1f}KB | "
                           f"æ€»è€—æ—¶: {time.time() - start_time:.2f}s")
                write_log(log_msg)
                return 0  # é€Ÿåº¦ä¸è¶³ï¼Œè¿”å›0

    except Exception as e:
        domain = get_domain(url)
        update_blacklist(domain)
        log_msg = (f"âŒ æµ‹é€Ÿå¤±è´¥: {url}\n"
                   f"   é”™è¯¯: {str(e)} | åŸŸå: {domain}")
        write_log(log_msg)
        return 0


def process_sources_optimized(sources):
    """ä¼˜åŒ–ç‰ˆæºå¤„ç†ï¼šæŒ‰åŸŸååˆ†ç»„æ£€æµ‹"""
    print("\nğŸ” å¼€å§‹ä¼˜åŒ–æ£€æµ‹æµç¨‹...")
    print(f"ğŸ“Š é€Ÿåº¦é˜ˆå€¼: {SPEED_THRESHOLD}KB/s")

    # æŒ‰åŸŸååˆ†ç»„
    domain_groups = group_sources_by_domain(sources)
    print(f"ğŸ“Š å°† {len(sources)} ä¸ªæºæŒ‰åŸŸååˆ†ç»„ä¸º {len(domain_groups)} ä¸ªç»„")

    processed = []
    test_results = {}  # åŸŸåæ£€æµ‹ç»“æœç¼“å­˜

    for domain_idx, (domain, group_sources) in enumerate(domain_groups.items(), 1):
        print(f"\nğŸ” å¤„ç†åŸŸåç»„ ({domain_idx}/{len(domain_groups)}): {domain}")
        print(f"   ç»„å†…åŒ…å« {len(group_sources)} ä¸ªé¢‘é“")

        # å¦‚æœè¯¥åŸŸåå·²ç»åœ¨ç¼“å­˜ä¸­ä¸”æœ‰ç»“æœï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜ç»“æœ
        if domain in domain_cache:
            speed_result = domain_cache[domain]  # ä¿®æ”¹å˜é‡åï¼Œé¿å…ä¸å‡½æ•°åå†²çª
            print(f"   âœ… ä½¿ç”¨ç¼“å­˜ç»“æœ: é€Ÿåº¦ {speed_result:.2f}KB/s")
        else:
            # é€‰æ‹©æµ‹è¯•é¢‘é“
            test_channel = select_test_channel(group_sources)
            if not test_channel:
                print(f"   âš ï¸ è¯¥ç»„æ— æœ‰æ•ˆæµ‹è¯•é¢‘é“ï¼Œè·³è¿‡")
                continue

            print(f"   ğŸ¯ é€‰æ‹©æµ‹è¯•é¢‘é“: {test_channel['name']},{test_channel['url']}")

            # æµ‹è¯•è¯¥é¢‘é“ - ä¿®æ”¹å˜é‡åï¼Œé¿å…ä¸å‡½æ•°åå†²çª
            speed_result = test_speed(test_channel['url'])
            domain_cache[domain] = speed_result
            test_results[domain] = speed_result

            if speed_result > SPEED_THRESHOLD:
                print(f"   âœ… åŸŸåæ£€æµ‹é€šè¿‡: é€Ÿåº¦ {speed_result:.2f}KB/s > {SPEED_THRESHOLD}KB/s")
            else:
                print(f"   âŒ åŸŸåæ£€æµ‹å¤±è´¥: é€Ÿåº¦ {speed_result:.2f}KB/s <= {SPEED_THRESHOLD}KB/s")

        # å¦‚æœæ£€æµ‹é€šè¿‡ï¼Œæ·»åŠ è¯¥åŸŸåä¸‹æ‰€æœ‰é¢‘é“
        if speed_result > SPEED_THRESHOLD:  # ä½¿ç”¨é€Ÿåº¦é˜ˆå€¼åˆ¤æ–­
            for source in group_sources:
                ip_type = get_ip_type(source['url'])
                processed.append((source['name'], source['url'], speed_result, ip_type))  # ä½¿ç”¨ä¿®æ”¹å˜é‡å
            print(f"   ğŸ“¥ æ·»åŠ  {len(group_sources)} ä¸ªé¢‘é“")
        else:
            print(f"   ğŸš« è·³è¿‡ {len(group_sources)} ä¸ªé¢‘é“ (é€Ÿåº¦ä¸è¶³)")

    # ä¿å­˜é»‘åå•æ›´æ–°
    if failed_domains:
        existing = set()
        if os.path.exists(BLACKLIST_FILE):
            # ä¿®å¤ç¼–ç é—®é¢˜
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            for encoding in encodings:
                try:
                    with open(BLACKLIST_FILE, 'r', encoding=encoding) as f:
                        existing = set(line.strip() for line in f)
                    break
                except UnicodeDecodeError:
                    continue
            else:
                with open(BLACKLIST_FILE, 'r', encoding='utf-8', errors='ignore') as f:
                    existing = set(line.strip() for line in f)

        new_domains = failed_domains - existing
        if new_domains:
            with open(BLACKLIST_FILE, 'a', encoding='utf-8') as f:
                for domain in new_domains:
                    f.write(f"{domain}\n")
            print(f"ğŸ†• æ–°å¢ {len(new_domains)} ä¸ªåŸŸååˆ°é»‘åå•")

    print(f"\nâœ… ä¼˜åŒ–æ£€æµ‹å®Œæˆï¼Œå…±å¤„ç† {len(processed)} ä¸ªé¢‘é“")
    return processed


def organize_channels(processed, alias_map, group_map):
    """æ•´ç†é¢‘é“æ•°æ®"""
    print("\nğŸ“š æ•´ç†é¢‘é“æ•°æ®...")
    organized = {'ipv4': OrderedDict(), 'ipv6': OrderedDict()}

    for name, url, speed, ip_type in processed:
        if ip_type not in ('ipv4', 'ipv6'):
            print(f"âš ï¸ å¼‚å¸¸IPç±»å‹: {ip_type}ï¼Œä½¿ç”¨ipv4ä»£æ›¿ â† {url}")
            ip_type = 'ipv4'

        std_name = alias_map.get(name, name)
        group = group_map.get(std_name, 'å…¶ä»–')

        if group not in organized[ip_type]:
            organized[ip_type][group] = OrderedDict()
        if std_name not in organized[ip_type][group]:
            organized[ip_type][group][std_name] = []

        organized[ip_type][group][std_name].append((url, speed))

    return organized


def finalize_output(organized, group_order, channel_order):
    """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶"""
    print("\nğŸ“‚ ç”Ÿæˆç»“æœæ–‡ä»¶...")
    for ip_type in ['ipv4', 'ipv6']:
        txt_lines = []
        m3u_lines = [
            '#EXTM3U x-tvg-url="https://gh.catmak.name/https://raw.githubusercontent.com/Guovin/iptv-api/refs/heads/master/output/epg/epg.gz"',
        ]

        # ç»Ÿè®¡é€šè¿‡çš„é¢‘é“æ•°é‡
        total_channels = 0

        # æŒ‰æ¨¡æ¿é¡ºåºå¤„ç†åˆ†ç»„
        for group in group_order:
            if group not in organized[ip_type]:
                continue

            txt_lines.append(f"{group},#genre#")

            # å¤„ç†æ¨¡æ¿é¢‘é“
            for channel in channel_order[group]:
                if channel not in organized[ip_type][group]:
                    continue

                # æŒ‰é€Ÿåº¦æ’åºï¼Œæœ€å¤šå–10ä¸ª
                urls = sorted(organized[ip_type][group][channel], key=lambda x: x[1], reverse=True)
                selected = [u[0] for u in urls[:10]]

                if selected:
                    # TXTæ ¼å¼ï¼šæ¯ä¸ªé“¾æ¥å•ç‹¬ä¸€è¡Œ
                    for url in selected:
                        txt_lines.append(f"{channel},{url}")

                    # M3Uæ ¼å¼
                    for url in selected:
                        m3u_lines.append(
                            f'#EXTINF:-1 tvg-name="{channel}" tvg-logo="https://gh.catmak.name/https://raw.githubusercontent.com/fanmingming/live/main/tv/{channel}.png" group-title="{group}",{channel}')
                        m3u_lines.append(url)

                    total_channels += 1

            # å¤„ç†é¢å¤–é¢‘é“
            extra = sorted(
                [c for c in organized[ip_type][group] if c not in channel_order[group]],
                key=lambda x: x.lower()
            )
            for channel in extra:
                urls = sorted(organized[ip_type][group][channel], key=lambda x: x[1], reverse=True)
                selected = [u[0] for u in urls[:10]]
                if selected:
                    for url in selected:
                        txt_lines.append(f"{channel},{url}")
                    for url in selected:
                        m3u_lines.append(f'#EXTINF:-1 tvg-name="{channel}" group-title="{group}",{channel}')
                        m3u_lines.append(url)

                    total_channels += 1

        # å¤„ç†å…¶ä»–åˆ†ç»„
        if 'å…¶ä»–' in organized[ip_type]:
            txt_lines.append("å…¶ä»–,#genre#")
            for channel in sorted(organized[ip_type]['å…¶ä»–'].keys(), key=lambda x: x.lower()):
                urls = sorted(organized[ip_type]['å…¶ä»–'][channel], key=lambda x: x[1], reverse=True)
                selected = [u[0] for u in urls[:10]]
                if selected:
                    for url in selected:
                        txt_lines.append(f"{channel},{url}")
                    for url in selected:
                        m3u_lines.append(f'#EXTINF:-1 tvg-name="{channel}" group-title="å…¶ä»–",{channel}')
                        m3u_lines.append(url)

                    total_channels += 1

        # å†™å…¥æ–‡ä»¶
        dir_path = IPV4_DIR if ip_type == 'ipv4' else IPV6_DIR
        with open(os.path.join(dir_path, 'result.txt'), 'w', encoding='utf-8') as f:
            f.write('\n'.join(txt_lines))
        with open(os.path.join(dir_path, 'result.m3u'), 'w', encoding='utf-8') as f:
            f.write('\n'.join(m3u_lines))

        print(f"  å·²ç”Ÿæˆ {ip_type.upper()} æ–‡ä»¶ â†’ {dir_path}")
        print(f"  é€šè¿‡é€Ÿåº¦é˜ˆå€¼çš„é¢‘é“æ•°é‡: {total_channels}")


if __name__ == '__main__':
    print("\n" + "=" * 50)
    print("ğŸ¬ IPTVç›´æ’­æºå¤„ç†è„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    print("=" * 50)
    print(f"âš¡ é€Ÿåº¦é˜ˆå€¼: {SPEED_THRESHOLD}KB/s")

    # åˆå§‹åŒ–è¿è¡Œè®¡æ•°å™¨å’Œé»‘åå•
    run_count = clear_blacklist_if_needed()
    print(f"ğŸ“Š å½“å‰è¿è¡Œæ¬¡æ•°: {run_count}")

    # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
    with open(SPEED_LOG, 'w', encoding='utf-8') as f:
        f.write(f"æµ‹é€Ÿæ—¥å¿— {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"é€Ÿåº¦é˜ˆå€¼: {SPEED_THRESHOLD}KB/s\n")

    # åˆå§‹åŒ–æ•°æ®
    alias_map, group_map, group_order, channel_order = parse_demo_file()
    sources = fetch_sources() + parse_local()
    blacklist = read_blacklist()

    # å¤„ç†æµç¨‹
    filtered = filter_sources(sources, blacklist)
    processed = process_sources_optimized(filtered)
    organized = organize_channels(processed, alias_map, group_map)
    finalize_output(organized, group_order, channel_order)

    print("\n" + "=" * 50)
    print("ğŸ‰ å¤„ç†å®Œæˆï¼ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³ output ç›®å½•")
    print("=" * 50)
